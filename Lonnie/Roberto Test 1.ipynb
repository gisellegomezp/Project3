{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /mnt/c/users/lonni/OneDrive/Project/Final Project/Lonnie\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files and directories: ['.git', '.gitignore', 'Giselle', 'Lonnie', 'Mike', 'Project Main', 'README.md']\n"
     ]
    }
   ],
   "source": [
    "files_and_dirs = os.listdir(\"../\")\n",
    "print(\"Files and directories:\", files_and_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\Project Main\\\\data\\\\Appliance_file_subset.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m..\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mProject Main\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mAppliance_file_subset.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124masin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparent_asin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhelpful_vote\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverified_purchase\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.10/site-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.10/site-packages/pandas/io/parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    275\u001b[0m         path_or_handle,\n\u001b[1;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    280\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.10/site-packages/pandas/io/parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.10/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..\\\\Project Main\\\\data\\\\Appliance_file_subset.parquet'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('..\\Project Main\\data\\Appliance_file_subset.parquet')\n",
    "\n",
    "df.drop(['images', 'asin', 'parent_asin', 'user_id',\n",
    "       'timestamp', 'helpful_vote', 'verified_purchase'], axis=1, inplace=True)\n",
    "\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lonni\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bert.modeling_tf_bert because of the following error (look up to see its traceback):\nNo module named 'keras.__internal__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\utils\\import_utils.py:1086\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:41\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     31\u001b[0m     TFBaseModelOutputWithPastAndCrossAttentions,\n\u001b[0;32m     32\u001b[0m     TFBaseModelOutputWithPoolingAndCrossAttentions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     TFTokenClassifierOutput,\n\u001b[0;32m     40\u001b[0m )\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     42\u001b[0m     TFCausalLanguageModelingLoss,\n\u001b[0;32m     43\u001b[0m     TFMaskedLanguageModelingLoss,\n\u001b[0;32m     44\u001b[0m     TFModelInputType,\n\u001b[0;32m     45\u001b[0m     TFMultipleChoiceLoss,\n\u001b[0;32m     46\u001b[0m     TFNextSentencePredictionLoss,\n\u001b[0;32m     47\u001b[0m     TFPreTrainedModel,\n\u001b[0;32m     48\u001b[0m     TFQuestionAnsweringLoss,\n\u001b[0;32m     49\u001b[0m     TFSequenceClassificationLoss,\n\u001b[0;32m     50\u001b[0m     TFTokenClassificationLoss,\n\u001b[0;32m     51\u001b[0m     get_initializer,\n\u001b[0;32m     52\u001b[0m     keras_serializable,\n\u001b[0;32m     53\u001b[0m     unpack_inputs,\n\u001b[0;32m     54\u001b[0m )\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_embeddings_within_bounds, shape_list, stable_softmax\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\modeling_tf_utils.py:76\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend \u001b[38;5;28;01mas\u001b[39;00m K\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasTensor\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_layer_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m call_context\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras.__internal__'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, TFBertForSequenceClassification\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\utils\\import_utils.py:1077\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1076\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1077\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\utils\\import_utils.py:1076\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1076\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1077\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\utils\\import_utils.py:1088\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1087\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1088\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1089\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1091\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.bert.modeling_tf_bert because of the following error (look up to see its traceback):\nNo module named 'keras.__internal__'"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "\n",
    "# Example preprocessing\n",
    "def encode_review(title, description):\n",
    "    review_text = title + \" \" + description\n",
    "    return tokenizer(review_text, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "\n",
    "# Example input data\n",
    "encoded_review = encode_review(\"Great product\", \"This product exceeded my expectations.\")\n",
    "output = model(encoded_review['input_ids'])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=5e-5), loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "# Fit the model on your training data\n",
    "model.fit(train_dataset, epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\lonni\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.6.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\lonni\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\lonni\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from keras) (1.26.2)\n",
      "Requirement already satisfied: rich in c:\\users\\lonni\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from keras) (13.9.2)\n",
      "Requirement already satisfied: namex in c:\\users\\lonni\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\lonni\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from keras) (3.12.1)\n",
      "Requirement already satisfied: optree in c:\\users\\lonni\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from keras) (0.13.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\lonni\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from keras) (0.4.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\lonni\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from keras) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\lonni\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from optree->keras) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\lonni\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lonni\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lonni\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "! pip install keras\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lonni\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.roberta.modeling_tf_roberta because of the following error (look up to see its traceback):\nNo module named 'keras.__internal__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\utils\\import_utils.py:1086\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py:39\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     30\u001b[0m     TFBaseModelOutputWithPastAndCrossAttentions,\n\u001b[0;32m     31\u001b[0m     TFBaseModelOutputWithPoolingAndCrossAttentions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     TFTokenClassifierOutput,\n\u001b[0;32m     38\u001b[0m )\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     40\u001b[0m     TFCausalLanguageModelingLoss,\n\u001b[0;32m     41\u001b[0m     TFMaskedLanguageModelingLoss,\n\u001b[0;32m     42\u001b[0m     TFModelInputType,\n\u001b[0;32m     43\u001b[0m     TFMultipleChoiceLoss,\n\u001b[0;32m     44\u001b[0m     TFPreTrainedModel,\n\u001b[0;32m     45\u001b[0m     TFQuestionAnsweringLoss,\n\u001b[0;32m     46\u001b[0m     TFSequenceClassificationLoss,\n\u001b[0;32m     47\u001b[0m     TFTokenClassificationLoss,\n\u001b[0;32m     48\u001b[0m     get_initializer,\n\u001b[0;32m     49\u001b[0m     keras_serializable,\n\u001b[0;32m     50\u001b[0m     unpack_inputs,\n\u001b[0;32m     51\u001b[0m )\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_embeddings_within_bounds, shape_list, stable_softmax\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\modeling_tf_utils.py:76\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend \u001b[38;5;28;01mas\u001b[39;00m K\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasTensor\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_layer_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m call_context\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras.__internal__'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RobertaTokenizer, TFRobertaForSequenceClassification\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\utils\\import_utils.py:1077\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1076\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1077\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\utils\\import_utils.py:1076\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1076\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1077\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\utils\\import_utils.py:1088\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1087\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1088\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1089\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1091\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.roberta.modeling_tf_roberta because of the following error (look up to see its traceback):\nNo module named 'keras.__internal__'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_parquet('..\\Project Main\\data\\Appliance_file_subset.parquet')\n",
    "df.drop(['images', 'asin', 'parent_asin', 'user_id', 'timestamp', 'helpful_vote', 'verified_purchase'], axis=1, inplace=True)\n",
    "\n",
    "# Combine the title and text into one feature\n",
    "df['combined_text'] = df['title'] + ' ' + df['text']\n",
    "\n",
    "# Prepare tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = TFRobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=5)\n",
    "\n",
    "# Tokenize the text data\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "# Prepare the inputs\n",
    "X = df['combined_text'].tolist()\n",
    "y = df['rating'].astype(int) - 1  # Ratings should be 0-based for the model (e.g., 0 for 1-star, 4 for 5-star)\n",
    "\n",
    "# Tokenize the text inputs\n",
    "X_tokenized = tokenize_function(X)\n",
    "\n",
    "# Convert tokenized input tensors to numpy arrays\n",
    "input_ids = X_tokenized['input_ids'].numpy()\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(input_ids, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=5e-5), loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=3,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f'Validation accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 17:13:55.530313: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-25 17:13:55.539387: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-25 17:13:55.548399: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-25 17:13:55.551578: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-25 17:13:55.558661: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-25 17:13:56.054441: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
=======
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Excellent.</td>\n",
       "      <td>Quality Product.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Works as expected.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Perfectly fit my Vornado</td>\n",
       "      <td>These filters are a perfect fit for my Vornado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Coffee Filter</td>\n",
       "      <td>This filter was just as good as the original o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Liked everything about k cups</td>\n",
       "      <td>Liked everything about k cups. These are much ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Fits and works perfectly on my refrigerator</td>\n",
       "      <td>It fits and works perfectly on my fridge with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>5.0</td>\n",
       "      <td>BEST FILTER OUT THERE!!!!!!!!</td>\n",
       "      <td>I had on of the gold mesh/plastic framed filte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Will never buy the aluminum ones again</td>\n",
       "      <td>Perfect to replace cheap aluminum ones on our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>5.0</td>\n",
       "      <td>great</td>\n",
       "      <td>nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Belts</td>\n",
       "      <td>Belts worked great and saved me alot of time a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rating                                        title  \\\n",
       "0         5.0                                   Excellent.   \n",
       "1         5.0                                   Five Stars   \n",
       "2         5.0                     Perfectly fit my Vornado   \n",
       "3         5.0                                Coffee Filter   \n",
       "4         5.0                Liked everything about k cups   \n",
       "...       ...                                          ...   \n",
       "49995     5.0  Fits and works perfectly on my refrigerator   \n",
       "49996     5.0                BEST FILTER OUT THERE!!!!!!!!   \n",
       "49997     5.0       Will never buy the aluminum ones again   \n",
       "49998     5.0                                        great   \n",
       "49999     5.0                                        Belts   \n",
       "\n",
       "                                                    text  \n",
       "0                                       Quality Product.  \n",
       "1                                     Works as expected.  \n",
       "2      These filters are a perfect fit for my Vornado...  \n",
       "3      This filter was just as good as the original o...  \n",
       "4      Liked everything about k cups. These are much ...  \n",
       "...                                                  ...  \n",
       "49995  It fits and works perfectly on my fridge with ...  \n",
       "49996  I had on of the gold mesh/plastic framed filte...  \n",
       "49997  Perfect to replace cheap aluminum ones on our ...  \n",
       "49998                                               nice  \n",
       "49999  Belts worked great and saved me alot of time a...  \n",
       "\n",
       "[50000 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>combined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Excellent.</td>\n",
       "      <td>Quality Product.</td>\n",
       "      <td>excellent quality product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Works as expected.</td>\n",
       "      <td>five starsworks expected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Perfectly fit my Vornado</td>\n",
       "      <td>These filters are a perfect fit for my Vornado...</td>\n",
       "      <td>perfectly fit vornadothese filters perfect fit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Coffee Filter</td>\n",
       "      <td>This filter was just as good as the original o...</td>\n",
       "      <td>coffee filterthis filter good original one cam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Liked everything about k cups</td>\n",
       "      <td>Liked everything about k cups. These are much ...</td>\n",
       "      <td>liked everything k cupsliked everything k cups...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Fits and works perfectly on my refrigerator</td>\n",
       "      <td>It fits and works perfectly on my fridge with ...</td>\n",
       "      <td>fits works perfectly refrigeratorit fits works...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>5.0</td>\n",
       "      <td>BEST FILTER OUT THERE!!!!!!!!</td>\n",
       "      <td>I had on of the gold mesh/plastic framed filte...</td>\n",
       "      <td>best filter gold mesh plastic framed filters y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Will never buy the aluminum ones again</td>\n",
       "      <td>Perfect to replace cheap aluminum ones on our ...</td>\n",
       "      <td>never buy aluminum ones againperfect replace c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>5.0</td>\n",
       "      <td>great</td>\n",
       "      <td>nice</td>\n",
       "      <td>greatnice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Belts</td>\n",
       "      <td>Belts worked great and saved me alot of time a...</td>\n",
       "      <td>beltsbelts worked great saved alot time money</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rating                                        title  \\\n",
       "0         5.0                                   Excellent.   \n",
       "1         5.0                                   Five Stars   \n",
       "2         5.0                     Perfectly fit my Vornado   \n",
       "3         5.0                                Coffee Filter   \n",
       "4         5.0                Liked everything about k cups   \n",
       "...       ...                                          ...   \n",
       "49995     5.0  Fits and works perfectly on my refrigerator   \n",
       "49996     5.0                BEST FILTER OUT THERE!!!!!!!!   \n",
       "49997     5.0       Will never buy the aluminum ones again   \n",
       "49998     5.0                                        great   \n",
       "49999     5.0                                        Belts   \n",
       "\n",
       "                                                    text  \\\n",
       "0                                       Quality Product.   \n",
       "1                                     Works as expected.   \n",
       "2      These filters are a perfect fit for my Vornado...   \n",
       "3      This filter was just as good as the original o...   \n",
       "4      Liked everything about k cups. These are much ...   \n",
       "...                                                  ...   \n",
       "49995  It fits and works perfectly on my fridge with ...   \n",
       "49996  I had on of the gold mesh/plastic framed filte...   \n",
       "49997  Perfect to replace cheap aluminum ones on our ...   \n",
       "49998                                               nice   \n",
       "49999  Belts worked great and saved me alot of time a...   \n",
       "\n",
       "                                           combined_text  \n",
       "0                              excellent quality product  \n",
       "1                               five starsworks expected  \n",
       "2      perfectly fit vornadothese filters perfect fit...  \n",
       "3      coffee filterthis filter good original one cam...  \n",
       "4      liked everything k cupsliked everything k cups...  \n",
       "...                                                  ...  \n",
       "49995  fits works perfectly refrigeratorit fits works...  \n",
       "49996  best filter gold mesh plastic framed filters y...  \n",
       "49997  never buy aluminum ones againperfect replace c...  \n",
       "49998                                          greatnice  \n",
       "49999      beltsbelts worked great saved alot time money  \n",
       "\n",
       "[50000 rows x 4 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mike/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/mike/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
>>>>>>> remotes/origin/main
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# define characters to remove and stop words\n",
    "regex = re.compile(\"[^a-zA-Z ]\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# function to clean text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = regex.sub(' ', text) # Substitute everything that is not a letter with an empty string\n",
    "    words = word_tokenize(text) # tokenize text\n",
    "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "# GPU check\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_parquet('../Project Main/data/Appliance_file_subset.parquet')\n",
    "df.drop(['images', 'asin', 'parent_asin', 'user_id', 'timestamp', 'helpful_vote', 'verified_purchase'], axis=1, inplace=True)\n",
    "#combined_text = df['title'] + df['text']\n",
    "df['combined_text'] = [preprocess_text(title + text) for title, text in zip(df['title'], df['text'])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1729890839.649129   89969 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1729890839.649888   89969 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1729890839.649926   89969 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "/home/laserlon/miniconda3/envs/dev/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
=======
      "/home/mike/anaconda3/envs/dev/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
>>>>>>> remotes/origin/main
      "  warnings.warn(\n",
      "I0000 00:00:1729890840.021286   89969 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1729890840.021338   89969 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1729890840.021352   89969 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1729890840.104784   89969 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1729890840.104847   89969 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-25 17:14:00.104853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1729890840.104881   89969 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-25 17:14:00.104896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5520 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
<<<<<<< HEAD
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1729890857.231459   90143 service.cc:146] XLA service 0x7fb468812560 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1729890857.231503   90143 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "2024-10-25 17:14:17.235231: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-10-25 17:14:17.243405: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8902\n",
      "I0000 00:00:1729890857.284984   90143 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 281/2500 [==>...........................] - ETA: 6:29 - loss: 1.0699 - accuracy: 0.6931"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Train the model (inside GPU context)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/GPU:1\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 56\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     63\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(val_dataset)\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:1229\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mfit)\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1228\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.10/site-packages/tf_keras/src/engine/training.py:1804\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1796\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1797\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1798\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1801\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1802\u001b[0m ):\n\u001b[1;32m   1803\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1804\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1806\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:869\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    868\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    875\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1567\u001b[0m   )\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
=======
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1730135352.506920   30210 service.cc:146] XLA service 0x7f0864511780 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1730135352.507005   30210 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "2024-10-28 13:09:12.519455: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-10-28 13:09:12.544582: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8902\n",
      "I0000 00:00:1730135352.604397   30210 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 551s 212ms/step - loss: 1.0232 - accuracy: 0.6973 - val_loss: 1.0097 - val_accuracy: 0.6968\n",
      "Epoch 2/3\n",
      "2500/2500 [==============================] - 526s 210ms/step - loss: 1.0101 - accuracy: 0.6988 - val_loss: 1.0069 - val_accuracy: 0.6968\n",
      "Epoch 3/3\n",
      "2500/2500 [==============================] - 538s 215ms/step - loss: 1.0086 - accuracy: 0.6988 - val_loss: 1.0083 - val_accuracy: 0.6968\n",
      "625/625 [==============================] - 45s 71ms/step - loss: 1.0083 - accuracy: 0.6968\n",
      "Validation accuracy: 0.6967999935150146\n"
>>>>>>> remotes/origin/main
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = TFRobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=5)\n",
    "\n",
    "# Tokenize the text data\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "# Prepare the inputs and labels\n",
    "X = df['combined_text'].tolist()\n",
    "y = tf.convert_to_tensor(df['rating'].astype(int).values - 1, dtype=tf.int32)  # Ensure labels are TensorFlow-compatible\n",
    "X_tokenized = tokenize_function(X)\n",
    "\n",
    "# Convert input_ids and attention_mask to NumPy arrays\n",
    "input_ids = X_tokenized['input_ids'].numpy()\n",
    "attention_mask = X_tokenized['attention_mask'].numpy()\n",
    "y = y.numpy()\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train_ids, X_val_ids, X_train_mask, X_val_mask, y_train, y_val = train_test_split(\n",
    "    input_ids, attention_mask, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert the data to a TensorFlow Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': X_train_ids, 'attention_mask': X_train_mask}, y_train)).batch(16)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': X_val_ids, 'attention_mask': X_val_mask}, y_val)).batch(16)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model (inside GPU context)\n",
    "with tf.device('/GPU:1'):\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=3\n",
    "    )\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(val_dataset)\n",
    "print(f'Validation accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../trained_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../trained_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('../../trained_model',save_format=\"tf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "df = pd.read_parquet('../Project Main/data/Appliance_file_subset.parquet')\n",
    "df.drop(['images', 'asin', 'parent_asin', 'user_id', 'timestamp', 'helpful_vote', 'verified_purchase'], axis=1, inplace=True)\n",
    "\n",
    "print(\"Available devices:\", tf.config.list_physical_devices())\n",
    "\n",
    "# Combine the title and text into one feature\n",
    "df['combined_text'] = df['title'] + ' ' + df['text']\n",
    " \n",
    "# Prepare tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Tokenize the text data\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    " \n",
    "# Prepare the inputs and labels\n",
    "X = df['combined_text'].tolist()\n",
    "y = tf.convert_to_tensor(df['rating'].astype(int).values - 1, dtype=tf.int32)  # Ensure labels are TensorFlow-compatible\n",
    " \n",
    "# Tokenize the text inputs\n",
    "X_tokenized = tokenize_function(X)\n",
    " \n",
    "# Convert input_ids and attention_mask to NumPy arrays\n",
    "input_ids = X_tokenized['input_ids'].numpy()\n",
    "attention_mask = X_tokenized['attention_mask'].numpy()\n",
    "y = y.numpy()  # Convert labels to numpy as well\n",
    " \n",
    "# Split the dataset into training and validation sets\n",
    "X_train_ids, X_val_ids, X_train_mask, X_val_mask, y_train, y_val = train_test_split(\n",
    "    input_ids, attention_mask, y, test_size=0.2, random_state=42\n",
    ")\n",
    " \n",
    "# Convert the data to a TensorFlow Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': X_train_ids, 'attention_mask': X_train_mask}, y_train)).batch(16)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': X_val_ids, 'attention_mask': X_val_mask}, y_val)).batch(16)\n",
    " \n",
    "# Compile the model\n",
    "# model.compile(\n",
    "#     optimizer=Adam(learning_rate=5e-5),\n",
    "#     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "model = TFRobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=5)\n",
    " \n",
    "model.compile(\n",
    "    optimizer='adam',  # Using a string identifier for Adam\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    " \n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=3\n",
    ")\n",
    " \n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(val_dataset)\n",
    "print(f'Validation accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 17:46:35.873019: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-25 17:46:35.879889: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-25 17:46:35.888225: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-25 17:46:35.890541: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-25 17:46:35.896985: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-25 17:46:36.384826: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Version: 12.3\n",
      "cuDNN Version: 8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"CUDA Version:\", tf.sysconfig.get_build_info()[\"cuda_version\"])\n",
    "print(\"cuDNN Version:\", tf.sysconfig.get_build_info()[\"cudnn_version\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Project Main/data/trained_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Project Main/data/trained_model/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the entire model to a file\n",
    "model.save('../Project Main/data/trained_model')\n",
    "\n",
    "# Load the model from the file\n",
    "loaded_model = tf.keras.models.load_model('../Project Main/data/trained_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>combined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13311</th>\n",
       "      <td>1.0</td>\n",
       "      <td>No apperent return mecenisums to correct the w...</td>\n",
       "      <td>Was the wrong size and am still trying to figu...</td>\n",
       "      <td>No apperent return mecenisums to correct the w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19978</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Don't buy this it doesn't work.</td>\n",
       "      <td>Bought two so far and neither one worked.  Plu...</td>\n",
       "      <td>Don't buy this it doesn't work. Bought two so ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10289</th>\n",
       "      <td>1.0</td>\n",
       "      <td>THIS FILTER IS NOT COMPATIBLE WITH DA97-08006A!</td>\n",
       "      <td>THIS FILTER IS NOT COMPATIBLE WITH DA97-08006A...</td>\n",
       "      <td>THIS FILTER IS NOT COMPATIBLE WITH DA97-08006A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42958</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Don’t buy these</td>\n",
       "      <td>These pods seemed like a good idea but don’t w...</td>\n",
       "      <td>Don’t buy these These pods seemed like a good ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49046</th>\n",
       "      <td>1.0</td>\n",
       "      <td>One Star</td>\n",
       "      <td>One star</td>\n",
       "      <td>One Star One star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3119</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Good product,</td>\n",
       "      <td>Good product,</td>\n",
       "      <td>Good product, Good product,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16013</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Excellence in product/setvice</td>\n",
       "      <td>Quick delivery- exactly what I needed to repla...</td>\n",
       "      <td>Excellence in product/setvice Quick delivery- ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48566</th>\n",
       "      <td>5.0</td>\n",
       "      <td>works great!</td>\n",
       "      <td>Perfect! That’s what I like.</td>\n",
       "      <td>works great! Perfect! That’s what I like.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13996</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Fit perfect</td>\n",
       "      <td>My stove looks nice again!</td>\n",
       "      <td>Fit perfect My stove looks nice again!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32367</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Taller filters</td>\n",
       "      <td>I like this product because I like to make ten...</td>\n",
       "      <td>Taller filters I like this product because I l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9355 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rating                                              title  \\\n",
       "13311     1.0  No apperent return mecenisums to correct the w...   \n",
       "19978     1.0                    Don't buy this it doesn't work.   \n",
       "10289     1.0    THIS FILTER IS NOT COMPATIBLE WITH DA97-08006A!   \n",
       "42958     1.0                                    Don’t buy these   \n",
       "49046     1.0                                           One Star   \n",
       "...       ...                                                ...   \n",
       "3119      5.0                                      Good product,   \n",
       "16013     5.0                      Excellence in product/setvice   \n",
       "48566     5.0                                       works great!   \n",
       "13996     5.0                                        Fit perfect   \n",
       "32367     5.0                                     Taller filters   \n",
       "\n",
       "                                                    text  \\\n",
       "13311  Was the wrong size and am still trying to figu...   \n",
       "19978  Bought two so far and neither one worked.  Plu...   \n",
       "10289  THIS FILTER IS NOT COMPATIBLE WITH DA97-08006A...   \n",
       "42958  These pods seemed like a good idea but don’t w...   \n",
       "49046                                           One star   \n",
       "...                                                  ...   \n",
       "3119                                       Good product,   \n",
       "16013  Quick delivery- exactly what I needed to repla...   \n",
       "48566                       Perfect! That’s what I like.   \n",
       "13996                         My stove looks nice again!   \n",
       "32367  I like this product because I like to make ten...   \n",
       "\n",
       "                                           combined_text  \n",
       "13311  No apperent return mecenisums to correct the w...  \n",
       "19978  Don't buy this it doesn't work. Bought two so ...  \n",
       "10289  THIS FILTER IS NOT COMPATIBLE WITH DA97-08006A...  \n",
       "42958  Don’t buy these These pods seemed like a good ...  \n",
       "49046                                  One Star One star  \n",
       "...                                                  ...  \n",
       "3119                         Good product, Good product,  \n",
       "16013  Excellence in product/setvice Quick delivery- ...  \n",
       "48566          works great! Perfect! That’s what I like.  \n",
       "13996             Fit perfect My stove looks nice again!  \n",
       "32367  Taller filters I like this product because I l...  \n",
       "\n",
       "[9355 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_parquet('../Project Main/data/Appliance_file_subset.parquet')\n",
    "df.drop(['images', 'asin', 'parent_asin', 'user_id', 'timestamp', 'helpful_vote', 'verified_purchase'], axis=1, inplace=True)\n",
    "\n",
    "# Combine the title and text into one feature\n",
    "df['combined_text'] = df['title'] + ' ' + df['text']\n",
    "\n",
    "min_count = df['rating'].value_counts().min()\n",
    "df = df.groupby('rating').sample(n=min_count, random_state=42)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laserlon/miniconda3/envs/dev/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Tokenize the text data\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    " \n",
    "# Prepare the inputs and labels\n",
    "X = df['combined_text'].tolist()\n",
    "y = tf.convert_to_tensor(df['rating'].astype(int).values - 1, dtype=tf.int32)  # Ensure labels are TensorFlow-compatible\n",
    " \n",
    "# Tokenize the text inputs\n",
    "X_tokenized = tokenize_function(X)\n",
    "\n",
    " # Convert input_ids and attention_mask to NumPy arrays\n",
    "input_ids = X_tokenized['input_ids'].numpy()\n",
    "attention_mask = X_tokenized['attention_mask'].numpy()\n",
    "y = y.numpy()  # Convert labels to numpy as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train_ids, X_val_ids, X_train_mask, X_val_mask, y_train, y_val = train_test_split(\n",
    "    input_ids, attention_mask, y, test_size=0.2, random_state=42\n",
    ")\n",
    " \n",
    "# Convert the data to a TensorFlow Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': X_train_ids, 'attention_mask': X_train_mask}, y_train)).batch(16)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': X_val_ids, 'attention_mask': X_val_mask}, y_val)).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1730155140.676030  398142 service.cc:146] XLA service 0x7f0ea0b086c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1730155140.676072  398142 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "2024-10-28 18:39:00.687656: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-10-28 18:39:00.712048: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8902\n",
      "I0000 00:00:1730155140.772917  398142 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 129s 234ms/step - loss: 1.6415 - accuracy: 0.1979 - val_loss: 1.6124 - val_accuracy: 0.1999\n",
      "Epoch 2/3\n",
      "468/468 [==============================] - 107s 228ms/step - loss: 1.6192 - accuracy: 0.2002 - val_loss: 1.6097 - val_accuracy: 0.1999\n",
      "Epoch 3/3\n",
      "468/468 [==============================] - 105s 224ms/step - loss: 1.6158 - accuracy: 0.1972 - val_loss: 1.6096 - val_accuracy: 0.1999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7f116a8ceaa0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TFRobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=5)\n",
    " \n",
    "model.compile(\n",
    "    optimizer='adam',  # Using a string identifier for Adam\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    " \n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 10s 81ms/step - loss: 1.6096 - accuracy: 0.1999\n",
      "Validation accuracy: 0.19989310204982758\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(val_dataset)\n",
    "print(f'Validation accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 4\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = TFRobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=5)\n",
    "\n",
    "# Tokenize the input\n",
    "input_text = \"Don't buy this it doesn't work. Bought two so far and neither one worked.  Plugged in, can hear it, but no ice.  Very expensive piece of junk.\"\n",
    "inputs = tokenizer(input_text, return_tensors='tf')\n",
    "\n",
    "# Make the prediction\n",
    "logits = model(**inputs).logits\n",
    "\n",
    "# Process the prediction\n",
    "predictions = tf.nn.softmax(logits, axis=-1)\n",
    "predicted_class = tf.argmax(predictions, axis=-1).numpy()\n",
    "\n",
    "print(f\"Predicted class: {predicted_class[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 4 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 4 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 4 : Actual class: 1.0\n",
      "Predicted class: 4 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 4 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 4 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 4 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 4 : Actual class: 5.0\n",
      "Predicted class: 4 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 4 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 4 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 4 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 4 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 4 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 4 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 4 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 4 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 4 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 4 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 4 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 4 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 4 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 4 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 4 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 4 : Actual class: 5.0\n",
      "Predicted class: 4 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 4 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 4 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 4 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 4 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 4 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 4 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 4 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 4 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 4 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 4 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 4 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 4 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 4 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 4 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 4 : Actual class: 5.0\n",
      "Predicted class: 4 : Actual class: 5.0\n",
      "Predicted class: 4 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 1.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 4 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 4 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 4 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 4 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 4 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 4 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 4 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 4 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 0 : Actual class: 3.0\n",
      "Predicted class: 4 : Actual class: 3.0\n",
      "Predicted class: 4 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 2.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 4.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 5.0\n",
      "Predicted class: 0 : Actual class: 4.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_rand = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "loop_counter= 0\n",
    "for index, row in df_rand.iterrows():\n",
    "    if loop_counter >= 500:\n",
    "        break\n",
    "    \n",
    "    # Increment the counter\n",
    "    loop_counter += 1\n",
    "    \n",
    "    input_text = row['combined_text']\n",
    "    inputs = tokenizer(input_text, return_tensors='tf')\n",
    "\n",
    "    # Make the prediction\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "    # Process the prediction\n",
    "    predictions = tf.nn.softmax(logits, axis=-1)\n",
    "    predicted_class = tf.argmax(predictions, axis=-1).numpy()\n",
    "    \n",
    "    print(f\"Predicted class: {predicted_class[0]} : Actual class: {row['rating']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't buy this it doesn't work. Bought two so far and neither one worked.  Plugged in, can hear it, but no ice.  Very expensive piece of junk.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1][\"combined_text\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
