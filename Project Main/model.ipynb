{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# define characters to remove and stop words\n",
    "regex = re.compile(\"[^a-zA-Z ]\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# function to clean text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = regex.sub(' ', text) # Substitute everything that is not a letter with an empty string\n",
    "    words = word_tokenize(text) # tokenize text\n",
    "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "# GPU check\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_parquet('../Project Main/data/Appliance_file_subset.parquet')\n",
    "df.drop(['images', 'asin', 'parent_asin', 'user_id', 'timestamp', 'helpful_vote', 'verified_purchase'], axis=1, inplace=True)\n",
    "#combined_text = df['title'] + df['text']\n",
    "df['combined_text'] = [preprocess_text(title + text) for title, text in zip(df['title'], df['text'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = TFRobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=5)\n",
    "\n",
    "# Tokenize the text data\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "# Prepare the inputs and labels\n",
    "X = df['combined_text'].tolist()\n",
    "y = tf.convert_to_tensor(df['rating'].astype(int).values - 1, dtype=tf.int32)  # Ensure labels are TensorFlow-compatible\n",
    "X_tokenized = tokenize_function(X)\n",
    "\n",
    "# Convert input_ids and attention_mask to NumPy arrays\n",
    "input_ids = X_tokenized['input_ids'].numpy()\n",
    "attention_mask = X_tokenized['attention_mask'].numpy()\n",
    "y = y.numpy()\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train_ids, X_val_ids, X_train_mask, X_val_mask, y_train, y_val = train_test_split(\n",
    "    input_ids, attention_mask, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert the data to a TensorFlow Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': X_train_ids, 'attention_mask': X_train_mask}, y_train)).batch(16)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': X_val_ids, 'attention_mask': X_val_mask}, y_val)).batch(16)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model (inside GPU context)\n",
    "with tf.device('/GPU:0'):\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=3\n",
    "    )\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(val_dataset)\n",
    "print(f'Validation accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save('../../trained_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
